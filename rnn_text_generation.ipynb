{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_text_generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZecLaCBaHUx"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout,BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "import os\n",
        "from pyvi import ViTokenizer\n",
        "import string\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PD0SZCIiGW9",
        "outputId": "191f6afd-9edd-4b94-9037-f6a510ea5846"
      },
      "source": [
        "!pip install pyvi"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi9w_X1YaSyG"
      },
      "source": [
        "filename = \"/content/hoaVangTrenCoXanh.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzp0N8RhgOs2"
      },
      "source": [
        "def clean_document(doc):\n",
        "    doc = ViTokenizer.tokenize(doc)\n",
        "    \n",
        "    tokens = doc.split() #Split in_to words\n",
        "    \n",
        "    table = str.maketrans('', '', string.punctuation.replace(\"_\", \"\")) #Remove all punctuation\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    \n",
        "    tokens = [word for word in tokens if word]\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQkdUJFHjNEJ"
      },
      "source": [
        "words = clean_document(raw_text)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JeiRIRGloZq",
        "outputId": "9167ea9a-aecd-409f-83eb-062a989ca939"
      },
      "source": [
        "words[:10]"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chú', 'đàn', 'bảo', 'tôi', 'con', 'xòe', 'tay', 'ra', 'cho', 'chú']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwKdCExilsvC"
      },
      "source": [
        "# đưa vào set lấy các trường hợp trùng \n",
        "words_set = sorted(list(set(words)))\n",
        "#lấy index\n",
        "char_to_int = dict((c, i) for i, c in enumerate(words_set))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(words_set))"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqx2cwLGl9zp",
        "outputId": "3b6b31f1-fc62-4e7a-b101-798d5cd72306"
      },
      "source": [
        "n_chars = len(words)\n",
        "n_vocab = len(words_set)\n",
        "print( \"Total Characters: \", n_chars)\n",
        "print( \"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  13654\n",
            "Total Vocab:  2095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0X_Qt-9ma2u",
        "outputId": "31024f6c-c4c9-46fc-fcfe-aa17d349e071"
      },
      "source": [
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = words[i:i + seq_length]\n",
        "\tseq_out = words[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "n_patterns"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3JxgD6tmzIj"
      },
      "source": [
        "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
        "#co dan X\n",
        "X = X / float((n_vocab))\n",
        "#one hot encoding\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZVeCkThm-JF"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-OgMg3-4ONS",
        "outputId": "5e39fd60-c88f-43db-8b19-17f455a9ee06"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_14 (LSTM)               (None, 100, 512)          1052672   \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 100, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_15 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2095)              1074735   \n",
            "=================================================================\n",
            "Total params: 4,228,655\n",
            "Trainable params: 4,227,631\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgJX-zIunhmX",
        "outputId": "0150b4b9-1ac1-4fa8-e9bc-26330e07d5dc"
      },
      "source": [
        "filepath=\"model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=200, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "106/106 [==============================] - 14s 86ms/step - loss: 7.5626 - accuracy: 0.0071\n",
            "\n",
            "Epoch 00001: loss improved from inf to 7.40094, saving model to model.hdf5\n",
            "Epoch 2/200\n",
            "106/106 [==============================] - 9s 87ms/step - loss: 6.6022 - accuracy: 0.0423\n",
            "\n",
            "Epoch 00002: loss improved from 7.40094 to 6.52845, saving model to model.hdf5\n",
            "Epoch 3/200\n",
            "106/106 [==============================] - 9s 89ms/step - loss: 6.3265 - accuracy: 0.0442\n",
            "\n",
            "Epoch 00003: loss improved from 6.52845 to 6.34200, saving model to model.hdf5\n",
            "Epoch 4/200\n",
            "106/106 [==============================] - 9s 90ms/step - loss: 6.1654 - accuracy: 0.0409\n",
            "\n",
            "Epoch 00004: loss improved from 6.34200 to 6.22266, saving model to model.hdf5\n",
            "Epoch 5/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 6.0589 - accuracy: 0.0424\n",
            "\n",
            "Epoch 00005: loss improved from 6.22266 to 6.12235, saving model to model.hdf5\n",
            "Epoch 6/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 5.9494 - accuracy: 0.0417\n",
            "\n",
            "Epoch 00006: loss improved from 6.12235 to 6.01554, saving model to model.hdf5\n",
            "Epoch 7/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 5.8272 - accuracy: 0.0393\n",
            "\n",
            "Epoch 00007: loss improved from 6.01554 to 5.89212, saving model to model.hdf5\n",
            "Epoch 8/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 5.6746 - accuracy: 0.0487\n",
            "\n",
            "Epoch 00008: loss improved from 5.89212 to 5.74739, saving model to model.hdf5\n",
            "Epoch 9/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 5.4591 - accuracy: 0.0507\n",
            "\n",
            "Epoch 00009: loss improved from 5.74739 to 5.56348, saving model to model.hdf5\n",
            "Epoch 10/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 5.2303 - accuracy: 0.0718\n",
            "\n",
            "Epoch 00010: loss improved from 5.56348 to 5.32639, saving model to model.hdf5\n",
            "Epoch 11/200\n",
            "106/106 [==============================] - 10s 90ms/step - loss: 4.8470 - accuracy: 0.0994\n",
            "\n",
            "Epoch 00011: loss improved from 5.32639 to 4.98383, saving model to model.hdf5\n",
            "Epoch 12/200\n",
            "106/106 [==============================] - 10s 90ms/step - loss: 4.4415 - accuracy: 0.1414\n",
            "\n",
            "Epoch 00012: loss improved from 4.98383 to 4.55011, saving model to model.hdf5\n",
            "Epoch 13/200\n",
            "106/106 [==============================] - 10s 90ms/step - loss: 4.0022 - accuracy: 0.1898\n",
            "\n",
            "Epoch 00013: loss improved from 4.55011 to 4.10052, saving model to model.hdf5\n",
            "Epoch 14/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 3.5258 - accuracy: 0.2546\n",
            "\n",
            "Epoch 00014: loss improved from 4.10052 to 3.61081, saving model to model.hdf5\n",
            "Epoch 15/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 3.0305 - accuracy: 0.3468\n",
            "\n",
            "Epoch 00015: loss improved from 3.61081 to 3.14097, saving model to model.hdf5\n",
            "Epoch 16/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 2.5805 - accuracy: 0.4179\n",
            "\n",
            "Epoch 00016: loss improved from 3.14097 to 2.67455, saving model to model.hdf5\n",
            "Epoch 17/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 2.1368 - accuracy: 0.5127\n",
            "\n",
            "Epoch 00017: loss improved from 2.67455 to 2.25655, saving model to model.hdf5\n",
            "Epoch 18/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 1.7610 - accuracy: 0.5899\n",
            "\n",
            "Epoch 00018: loss improved from 2.25655 to 1.86249, saving model to model.hdf5\n",
            "Epoch 19/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 1.4009 - accuracy: 0.6676\n",
            "\n",
            "Epoch 00019: loss improved from 1.86249 to 1.51055, saving model to model.hdf5\n",
            "Epoch 20/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 1.1305 - accuracy: 0.7289\n",
            "\n",
            "Epoch 00020: loss improved from 1.51055 to 1.20756, saving model to model.hdf5\n",
            "Epoch 21/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.8943 - accuracy: 0.7792\n",
            "\n",
            "Epoch 00021: loss improved from 1.20756 to 0.96500, saving model to model.hdf5\n",
            "Epoch 22/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.6895 - accuracy: 0.8332\n",
            "\n",
            "Epoch 00022: loss improved from 0.96500 to 0.75105, saving model to model.hdf5\n",
            "Epoch 23/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.5306 - accuracy: 0.8719\n",
            "\n",
            "Epoch 00023: loss improved from 0.75105 to 0.59244, saving model to model.hdf5\n",
            "Epoch 24/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.4120 - accuracy: 0.9025\n",
            "\n",
            "Epoch 00024: loss improved from 0.59244 to 0.44807, saving model to model.hdf5\n",
            "Epoch 25/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.3260 - accuracy: 0.9274\n",
            "\n",
            "Epoch 00025: loss improved from 0.44807 to 0.35921, saving model to model.hdf5\n",
            "Epoch 26/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.2618 - accuracy: 0.9442\n",
            "\n",
            "Epoch 00026: loss improved from 0.35921 to 0.29066, saving model to model.hdf5\n",
            "Epoch 27/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.2149 - accuracy: 0.9579\n",
            "\n",
            "Epoch 00027: loss improved from 0.29066 to 0.23602, saving model to model.hdf5\n",
            "Epoch 28/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.1824 - accuracy: 0.9608\n",
            "\n",
            "Epoch 00028: loss improved from 0.23602 to 0.19184, saving model to model.hdf5\n",
            "Epoch 29/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.1462 - accuracy: 0.9720\n",
            "\n",
            "Epoch 00029: loss improved from 0.19184 to 0.15710, saving model to model.hdf5\n",
            "Epoch 30/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.1153 - accuracy: 0.9793\n",
            "\n",
            "Epoch 00030: loss improved from 0.15710 to 0.13265, saving model to model.hdf5\n",
            "Epoch 31/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.1099 - accuracy: 0.9790\n",
            "\n",
            "Epoch 00031: loss improved from 0.13265 to 0.11716, saving model to model.hdf5\n",
            "Epoch 32/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0888 - accuracy: 0.9872\n",
            "\n",
            "Epoch 00032: loss improved from 0.11716 to 0.09185, saving model to model.hdf5\n",
            "Epoch 33/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0860 - accuracy: 0.9814\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.09185\n",
            "Epoch 34/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0775 - accuracy: 0.9858\n",
            "\n",
            "Epoch 00034: loss improved from 0.09185 to 0.08572, saving model to model.hdf5\n",
            "Epoch 35/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0699 - accuracy: 0.9867\n",
            "\n",
            "Epoch 00035: loss improved from 0.08572 to 0.07628, saving model to model.hdf5\n",
            "Epoch 36/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0673 - accuracy: 0.9883\n",
            "\n",
            "Epoch 00036: loss improved from 0.07628 to 0.07008, saving model to model.hdf5\n",
            "Epoch 37/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0532 - accuracy: 0.9920\n",
            "\n",
            "Epoch 00037: loss improved from 0.07008 to 0.06228, saving model to model.hdf5\n",
            "Epoch 38/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0580 - accuracy: 0.9889\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.06228\n",
            "Epoch 39/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0706 - accuracy: 0.9855\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.06228\n",
            "Epoch 40/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0636 - accuracy: 0.9867\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.06228\n",
            "Epoch 41/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0550 - accuracy: 0.9893\n",
            "\n",
            "Epoch 00041: loss improved from 0.06228 to 0.05865, saving model to model.hdf5\n",
            "Epoch 42/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0507 - accuracy: 0.9916\n",
            "\n",
            "Epoch 00042: loss improved from 0.05865 to 0.05648, saving model to model.hdf5\n",
            "Epoch 43/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0427 - accuracy: 0.9934\n",
            "\n",
            "Epoch 00043: loss improved from 0.05648 to 0.04805, saving model to model.hdf5\n",
            "Epoch 44/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0445 - accuracy: 0.9914\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.04805\n",
            "Epoch 45/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0492 - accuracy: 0.9901\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.04805\n",
            "Epoch 46/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 0.0532 - accuracy: 0.9901\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.04805\n",
            "Epoch 47/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 0.0519 - accuracy: 0.9891\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.04805\n",
            "Epoch 48/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 0.0535 - accuracy: 0.9885\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.04805\n",
            "Epoch 49/200\n",
            "106/106 [==============================] - 10s 91ms/step - loss: 0.0652 - accuracy: 0.9851\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.04805\n",
            "Epoch 50/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0734 - accuracy: 0.9817\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.04805\n",
            "Epoch 51/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0701 - accuracy: 0.9830\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.04805\n",
            "Epoch 52/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0711 - accuracy: 0.9818\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.04805\n",
            "Epoch 53/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0599 - accuracy: 0.9854\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.04805\n",
            "Epoch 54/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0478 - accuracy: 0.9884\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.04805\n",
            "Epoch 55/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0451 - accuracy: 0.9883\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.04805\n",
            "Epoch 56/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0519 - accuracy: 0.9869\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.04805\n",
            "Epoch 57/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0365 - accuracy: 0.9921\n",
            "\n",
            "Epoch 00057: loss improved from 0.04805 to 0.03787, saving model to model.hdf5\n",
            "Epoch 58/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0283 - accuracy: 0.9947\n",
            "\n",
            "Epoch 00058: loss improved from 0.03787 to 0.03309, saving model to model.hdf5\n",
            "Epoch 59/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0379 - accuracy: 0.9916\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.03309\n",
            "Epoch 60/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0442 - accuracy: 0.9892\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.03309\n",
            "Epoch 61/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0422 - accuracy: 0.9914\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.03309\n",
            "Epoch 62/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0364 - accuracy: 0.9902\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.03309\n",
            "Epoch 63/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0462 - accuracy: 0.9900\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.03309\n",
            "Epoch 64/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0481 - accuracy: 0.9899\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.03309\n",
            "Epoch 65/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0463 - accuracy: 0.9878\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.03309\n",
            "Epoch 66/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0547 - accuracy: 0.9856\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.03309\n",
            "Epoch 67/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0498 - accuracy: 0.9877\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.03309\n",
            "Epoch 68/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0404 - accuracy: 0.9902\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.03309\n",
            "Epoch 69/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0429 - accuracy: 0.9887\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.03309\n",
            "Epoch 70/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0259 - accuracy: 0.9943\n",
            "\n",
            "Epoch 00070: loss improved from 0.03309 to 0.02920, saving model to model.hdf5\n",
            "Epoch 71/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0209 - accuracy: 0.9953\n",
            "\n",
            "Epoch 00071: loss improved from 0.02920 to 0.02508, saving model to model.hdf5\n",
            "Epoch 72/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0278 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.02508\n",
            "Epoch 73/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0237 - accuracy: 0.9938\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.02508\n",
            "Epoch 74/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0326 - accuracy: 0.9933\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.02508\n",
            "Epoch 75/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0578 - accuracy: 0.9856\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.02508\n",
            "Epoch 76/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0453 - accuracy: 0.9886\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.02508\n",
            "Epoch 77/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0442 - accuracy: 0.9904\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.02508\n",
            "Epoch 78/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0372 - accuracy: 0.9895\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.02508\n",
            "Epoch 79/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0402 - accuracy: 0.9894\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.02508\n",
            "Epoch 80/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0394 - accuracy: 0.9901\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.02508\n",
            "Epoch 81/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0377 - accuracy: 0.9903\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.02508\n",
            "Epoch 82/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0336 - accuracy: 0.9918\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.02508\n",
            "Epoch 83/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0365 - accuracy: 0.9903\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.02508\n",
            "Epoch 84/200\n",
            "106/106 [==============================] - 10s 92ms/step - loss: 0.0320 - accuracy: 0.9901\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.02508\n",
            "Epoch 85/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0340 - accuracy: 0.9920\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02508\n",
            "Epoch 86/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0247 - accuracy: 0.9935\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02508\n",
            "Epoch 87/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0193 - accuracy: 0.9949\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.02508\n",
            "Epoch 88/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0190 - accuracy: 0.9961\n",
            "\n",
            "Epoch 00088: loss improved from 0.02508 to 0.02177, saving model to model.hdf5\n",
            "Epoch 89/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0172 - accuracy: 0.9960\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02177\n",
            "Epoch 90/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0254 - accuracy: 0.9929\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02177\n",
            "Epoch 91/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0246 - accuracy: 0.9936\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02177\n",
            "Epoch 92/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0335 - accuracy: 0.9904\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02177\n",
            "Epoch 93/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0296 - accuracy: 0.9932\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02177\n",
            "Epoch 94/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0238 - accuracy: 0.9923\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02177\n",
            "Epoch 95/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0270 - accuracy: 0.9935\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.02177\n",
            "Epoch 96/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0225 - accuracy: 0.9940\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.02177\n",
            "Epoch 97/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0249 - accuracy: 0.9932\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.02177\n",
            "Epoch 98/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0185 - accuracy: 0.9949\n",
            "\n",
            "Epoch 00098: loss improved from 0.02177 to 0.01966, saving model to model.hdf5\n",
            "Epoch 99/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0224 - accuracy: 0.9963\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.01966\n",
            "Epoch 100/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0154 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00100: loss improved from 0.01966 to 0.01754, saving model to model.hdf5\n",
            "Epoch 101/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0187 - accuracy: 0.9958\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.01754\n",
            "Epoch 102/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0362 - accuracy: 0.9912\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.01754\n",
            "Epoch 103/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0262 - accuracy: 0.9919\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.01754\n",
            "Epoch 104/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0400 - accuracy: 0.9899\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.01754\n",
            "Epoch 105/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0303 - accuracy: 0.9902\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.01754\n",
            "Epoch 106/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0449 - accuracy: 0.9875\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.01754\n",
            "Epoch 107/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0371 - accuracy: 0.9886\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.01754\n",
            "Epoch 108/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0335 - accuracy: 0.9901\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.01754\n",
            "Epoch 109/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0269 - accuracy: 0.9936\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.01754\n",
            "Epoch 110/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0337 - accuracy: 0.9899\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.01754\n",
            "Epoch 111/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0280 - accuracy: 0.9931\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.01754\n",
            "Epoch 112/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0272 - accuracy: 0.9917\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.01754\n",
            "Epoch 113/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0254 - accuracy: 0.9940\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.01754\n",
            "Epoch 114/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0212 - accuracy: 0.9943\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.01754\n",
            "Epoch 115/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0163 - accuracy: 0.9966\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.01754\n",
            "Epoch 116/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0192 - accuracy: 0.9947\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.01754\n",
            "Epoch 117/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0216 - accuracy: 0.9939\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.01754\n",
            "Epoch 118/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0192 - accuracy: 0.9944\n",
            "\n",
            "Epoch 00118: loss improved from 0.01754 to 0.01645, saving model to model.hdf5\n",
            "Epoch 119/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0086 - accuracy: 0.9973\n",
            "\n",
            "Epoch 00119: loss improved from 0.01645 to 0.01097, saving model to model.hdf5\n",
            "Epoch 120/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0134 - accuracy: 0.9970\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.01097\n",
            "Epoch 121/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0144 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.01097\n",
            "Epoch 122/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0139 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.01097\n",
            "Epoch 123/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0212 - accuracy: 0.9947\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.01097\n",
            "Epoch 124/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0371 - accuracy: 0.9912\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.01097\n",
            "Epoch 125/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.1933 - accuracy: 0.9464\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.01097\n",
            "Epoch 126/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0682 - accuracy: 0.9793\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.01097\n",
            "Epoch 127/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0264 - accuracy: 0.9914\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.01097\n",
            "Epoch 128/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0158 - accuracy: 0.9954\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.01097\n",
            "Epoch 129/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0140 - accuracy: 0.9969\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.01097\n",
            "Epoch 130/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0109 - accuracy: 0.9971\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.01097\n",
            "Epoch 131/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0120 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.01097\n",
            "Epoch 132/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0073 - accuracy: 0.9983\n",
            "\n",
            "Epoch 00132: loss improved from 0.01097 to 0.00862, saving model to model.hdf5\n",
            "Epoch 133/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0069 - accuracy: 0.9983\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.00862\n",
            "Epoch 134/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0056 - accuracy: 0.9985\n",
            "\n",
            "Epoch 00134: loss improved from 0.00862 to 0.00635, saving model to model.hdf5\n",
            "Epoch 135/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0060 - accuracy: 0.9986\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.00635\n",
            "Epoch 136/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0078 - accuracy: 0.9980\n",
            "\n",
            "Epoch 00136: loss improved from 0.00635 to 0.00625, saving model to model.hdf5\n",
            "Epoch 137/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0045 - accuracy: 0.9985\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.00625\n",
            "Epoch 138/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0084 - accuracy: 0.9974\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.00625\n",
            "Epoch 139/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0068 - accuracy: 0.9979\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.00625\n",
            "Epoch 140/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0084 - accuracy: 0.9980\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.00625\n",
            "Epoch 141/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0106 - accuracy: 0.9973\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.00625\n",
            "Epoch 142/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0147 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.00625\n",
            "Epoch 143/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0139 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.00625\n",
            "Epoch 144/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0255 - accuracy: 0.9931\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.00625\n",
            "Epoch 145/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0417 - accuracy: 0.9860\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.00625\n",
            "Epoch 146/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0466 - accuracy: 0.9872\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.00625\n",
            "Epoch 147/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0463 - accuracy: 0.9864\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.00625\n",
            "Epoch 148/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0270 - accuracy: 0.9925\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.00625\n",
            "Epoch 149/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0282 - accuracy: 0.9923\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.00625\n",
            "Epoch 150/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0144 - accuracy: 0.9955\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.00625\n",
            "Epoch 151/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0132 - accuracy: 0.9959\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.00625\n",
            "Epoch 152/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0111 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.00625\n",
            "Epoch 153/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.00625\n",
            "Epoch 154/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0155 - accuracy: 0.9961\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.00625\n",
            "Epoch 155/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0116 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.00625\n",
            "Epoch 156/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0150 - accuracy: 0.9959\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.00625\n",
            "Epoch 157/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0201 - accuracy: 0.9950\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.00625\n",
            "Epoch 158/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0142 - accuracy: 0.9967\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.00625\n",
            "Epoch 159/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0120 - accuracy: 0.9966\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.00625\n",
            "Epoch 160/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0142 - accuracy: 0.9959\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.00625\n",
            "Epoch 161/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0244 - accuracy: 0.9938\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.00625\n",
            "Epoch 162/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0276 - accuracy: 0.9905\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.00625\n",
            "Epoch 163/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0363 - accuracy: 0.9893\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.00625\n",
            "Epoch 164/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0299 - accuracy: 0.9902\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.00625\n",
            "Epoch 165/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0264 - accuracy: 0.9928\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.00625\n",
            "Epoch 166/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0274 - accuracy: 0.9920\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.00625\n",
            "Epoch 167/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0277 - accuracy: 0.9928\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.00625\n",
            "Epoch 168/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0235 - accuracy: 0.9922\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.00625\n",
            "Epoch 169/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0201 - accuracy: 0.9933\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.00625\n",
            "Epoch 170/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0210 - accuracy: 0.9947\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.00625\n",
            "Epoch 171/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0171 - accuracy: 0.9949\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.00625\n",
            "Epoch 172/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0153 - accuracy: 0.9958\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.00625\n",
            "Epoch 173/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0162 - accuracy: 0.9944\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.00625\n",
            "Epoch 174/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0146 - accuracy: 0.9966\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.00625\n",
            "Epoch 175/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0146 - accuracy: 0.9957\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.00625\n",
            "Epoch 176/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0090 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.00625\n",
            "Epoch 177/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0141 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.00625\n",
            "Epoch 178/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0110 - accuracy: 0.9967\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.00625\n",
            "Epoch 179/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0094 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.00625\n",
            "Epoch 180/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0090 - accuracy: 0.9977\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.00625\n",
            "Epoch 181/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0074 - accuracy: 0.9981\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.00625\n",
            "Epoch 182/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0081 - accuracy: 0.9974\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.00625\n",
            "Epoch 183/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0142 - accuracy: 0.9957\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.00625\n",
            "Epoch 184/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0186 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.00625\n",
            "Epoch 185/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0220 - accuracy: 0.9941\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.00625\n",
            "Epoch 186/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0238 - accuracy: 0.9926\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.00625\n",
            "Epoch 187/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0355 - accuracy: 0.9903\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.00625\n",
            "Epoch 188/200\n",
            "106/106 [==============================] - 10s 95ms/step - loss: 0.0413 - accuracy: 0.9880\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.00625\n",
            "Epoch 189/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0378 - accuracy: 0.9882\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.00625\n",
            "Epoch 190/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0341 - accuracy: 0.9890\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.00625\n",
            "Epoch 191/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0266 - accuracy: 0.9921\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.00625\n",
            "Epoch 192/200\n",
            "106/106 [==============================] - 10s 93ms/step - loss: 0.0187 - accuracy: 0.9949\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.00625\n",
            "Epoch 193/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0131 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.00625\n",
            "Epoch 194/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0146 - accuracy: 0.9949\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.00625\n",
            "Epoch 195/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0130 - accuracy: 0.9960\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.00625\n",
            "Epoch 196/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0082 - accuracy: 0.9979\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.00625\n",
            "Epoch 197/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0089 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.00625\n",
            "Epoch 198/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0105 - accuracy: 0.9964\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.00625\n",
            "Epoch 199/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0326 - accuracy: 0.9913\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.00625\n",
            "Epoch 200/200\n",
            "106/106 [==============================] - 10s 94ms/step - loss: 0.0363 - accuracy: 0.9893\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.00625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbf0ea5710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sncn6eShp3AP",
        "outputId": "b12a3ceb-dce5-478d-cd8d-83afe0e1bb27"
      },
      "source": [
        "filename = 'model.hdf5'\n",
        "model.load_weights(filename)\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ' '.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "for i in range(50):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    print(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" nói rất hay điều đơn_giản thế_mà lâu_nay chú không nghĩ ra ha_ha hay quá thế là tôi vẫn có năm cái hoa_tay bà_con ơi tôi xác_nhận với các bạn là chú đàn vẫn còn năm cái hoa_tay dù không ai được nhìn thấy chúngnữa vì chú vẽ rất đẹp dưới mắt thằng tường thì tôi là người vẽ đẹp nhất nhưng dưới mắt tôi thì chú đàn mới là người số một các bài_tập vẽ của tôi lúc mê chơi tôi vẫn thường nhờ chú vẽ giùm và bao giờ các tranh vẽ của chú cũng đạt điểm_cao nhất lớp \"\n",
            "hồi\n",
            "đó\n",
            "nhà\n",
            "tôi\n",
            "ở\n",
            "cạnh\n",
            "đường\n",
            "quốc_lộ\n",
            "sau\n",
            "nhà\n",
            "là\n",
            "một\n",
            "nghĩa_trang\n",
            "rộng\n",
            "mênh_mông\n",
            "nghĩa_trang\n",
            "rộng\n",
            "đến\n",
            "mức\n",
            "ngay\n",
            "trong\n",
            "vườn\n",
            "nhà\n",
            "tôi\n",
            "cũng\n",
            "có\n",
            "hai\n",
            "ngôi\n",
            "mộ\n",
            "hoang\n",
            "trú_ngụ\n",
            "từ\n",
            "rất\n",
            "lâu\n",
            "trước\n",
            "khi\n",
            "gia_đình\n",
            "tôi\n",
            "dọn\n",
            "đến\n",
            "buổi\n",
            "tối\n",
            "ngồi\n",
            "trong\n",
            "nhà\n",
            "nhìn\n",
            "ra\n",
            "những\n",
            "đốm\n",
            "nhang\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzgVLTPb20qz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}